import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.messages import AIMessageChunk
from dotenv import load_dotenv, find_dotenv
import os
import time  # Importado time para a animaÃ§Ã£o da barra de progresso
# Importa a funÃ§Ã£o para obter o limite de tokens
from modelos_tokens import obter_limite_tokens, limitar_texto_por_tokens, tokens_para_paginas_a4
from langchain_google_genai import ChatGoogleGenerativeAI

import pandas as pd
from PyPDF2 import PdfReader
import docx
import openpyxl

import gerenciador_conversas as gc
from gpt5_handler import GPT5Handler

# Carrega variÃ¡veis de ambiente no inÃ­cio
_ = load_dotenv(find_dotenv())

# No inÃ­cio do seu app.py, apÃ³s os imports, vocÃª pode querer limpar
# a informaÃ§Ã£o de modelo carregado se nÃ£o for uma recarga de uma conversa
if "modelo_carregado_info" not in st.session_state:
    st.session_state.modelo_carregado_info = None

# Habilita cache em memÃ³ria para LLMs


def habilitar_memory_cache():
    """Habilita o cache em memÃ³ria para as respostas do LLM."""
    from langchain_community.cache import InMemoryCache
    from langchain.globals import set_llm_cache
    set_llm_cache(InMemoryCache())


habilitar_memory_cache()

# ConfiguraÃ§Ã£o da pÃ¡gina do Streamlit
st.set_page_config(page_title="Chat com IA", layout="centered")
st.title("Converse com a IA!")

# DefiniÃ§Ã£o dos modelos disponÃ­veis (locais e OpenAI)
modelos_locais = {
    "deepseek-r1:8b": {"type": "ollama"},
    "deepseek-coder:6.7b": {"type": "ollama"},
    "llama3.1": {"type": "ollama"},
    "llama3.2": {"type": "ollama"},
    "qwen3:8b": {"type": "ollama"},
    "qwen2.5:7b": {"type": "ollama"},
    "qwen2.5-coder:7b": {"type": "ollama"},
    "gemma3:4b": {"type": "ollama"},
    "mistral": {"type": "ollama"},
    "phi4-mini-reasoning:3.8b": {"type": "ollama"},
    "phi4-mini:3.8b": {"type": "ollama"},  # recurso de chamada de funÃ§Ã£o
    "phi3.5:3.8b": {"type": "ollama"},
    "granite3.3:8b": {"type": "ollama"}
}

modelos_openai = {
    "gpt-5": {"type": "gpt5"},
    "gpt-5-mini": {"type": "gpt5"},
    "gpt-5-nano": {"type": "gpt5"},
    "gpt-5-2025-08-07": {"type": "openai"},
    "gpt-5-mini-2025-08-07": {"type": "openai"},
    "gpt-5-nano-2025-08-07": {"type": "openai"},
    "gpt-4.1-2025-04-14": {"type": "openai"},
    "gpt-4.1-mini-2025-04-14": {"type": "openai"},
    "gpt-4.1-nano-2025-04-14": {"type": "openai"},
    "gpt-4o-mini-2024-07-18": {"type": "openai"},
    "o3-2025-04-16": {"type": "openai"},
    "o4-mini-2025-04-16": {"type": "openai"},
    "gpt-4o": {"type": "openai"},
    "gpt-4": {"type": "openai"},
    "gpt-4-turbo": {"type": "openai"},
    "gpt-3.5-turbo": {"type": "openai"},
    "gpt-4o-mini-search-preview-2025-03-11": {"type": "openai"}
}

modelos_gemini = {
    "gemini-1.5-flash": {"type": "gemini"},
    "gemini-1.5-pro": {"type": "gemini"},
    "gemini-2.0-flash": {"type": "gemini"},
    "gemini-2.0-flash-lite": {"type": "gemini"},
    "gemini-2.5-pro": {"type": "gemini"},
    "gemini-2.5-flash": {"type": "gemini"},
    "gemini-2.5-flash-lite-preview-06-17": {"type": "gemini"},

}

# Combina todos os modelos em um dicionÃ¡rio
todos_modelos = {
    **{"Local: " + k: v for k, v in modelos_locais.items()},
    **{"OpenAI: " + k: v for k, v in modelos_openai.items()},
    **{"Gemini: " + k: v for k, v in modelos_gemini.items()}
}


# FunÃ§Ã£o para carregar o modelo LLM (com cache)
# Esta funÃ§Ã£o aceita o parÃ¢metro 'temperature'


@st.cache_resource
def load_model(model_name, model_info, temperature=0.5):
    try:
        if model_info["type"] == "ollama":
            return ChatOllama(model=model_name, base_url='http://localhost:11434', temperature=temperature)
        elif model_info["type"] == "openai":
            if not os.getenv("OPENAI_API_KEY"):
                st.error("Chave API da OpenAI nÃ£o encontrada no arquivo .env")
                return None
            return ChatOpenAI(model=model_name, temperature=temperature)
        elif model_info["type"] == "gpt5":
            if not os.getenv("OPENAI_API_KEY"):
                st.error("Chave API da OpenAI nÃ£o encontrada no arquivo .env")
                return None
            # Retorna o handler do GPT-5 ao invÃ©s do ChatOpenAI padrÃ£o
            return GPT5Handler()
        elif model_info["type"] == "gemini":
            gemini_api_key = os.getenv("GEMINI_API_KEY")
            if not os.getenv("GEMINI_API_KEY"):
                st.error("Chave API do Google nÃ£o encontrada no arquivo .env")
                return None
            return ChatGoogleGenerativeAI(
                model=model_name,
                temperature=temperature,
                max_tokens=None,
                max_retries=2,
                api_key=gemini_api_key
            )
    except Exception as e:
        st.error(f"Erro ao carregar o modelo {model_name}: {e}")
        return None

# FunÃ§Ã£o para limpar o histÃ³rico de mensagens


def limpar_historico():
    """Limpa o histÃ³rico de mensagens e a resposta completa na sessÃ£o."""
    st.session_state["mensagens"] = []
    st.session_state["resposta_completa"] = ""

    if "modelo_carregado_info" in st.session_state:
        st.session_state.modelo_carregado_info = None


# InicializaÃ§Ã£o do histÃ³rico e variÃ¡vel de resposta na sessÃ£o
if "mensagens" not in st.session_state:
    st.session_state["mensagens"] = []
if "resposta_completa" not in st.session_state:
    st.session_state["resposta_completa"] = ""
# Inicializa a variÃ¡vel de estado para parar a geraÃ§Ã£o
if "parar_geracao" not in st.session_state:
    st.session_state["parar_geracao"] = False


# ========== SIDEBAR ==========
with st.sidebar:
    st.title("ConfiguraÃ§Ãµes")
    # Selectbox para escolher o modelo
    # Usando uma key Ãºnica para evitar o erro de duplicidade
    modelo_selecionado_key = st.sidebar.selectbox(
        "Escolha o modelo", options=list(todos_modelos.keys()), key="modelo_selector")

    # Verificar se Ã© GPT-5 primeiro para desabilitar temperatura
    modelo_selecionado_temp = modelo_selecionado_key.split(":", 1)[1].strip()
    is_gpt5_temp = any(gpt5_model in modelo_selecionado_temp for gpt5_model in ["gpt-5", "gpt-5-mini", "gpt-5-nano"])
    
    # Slider para controlar a temperatura do modelo
    if not is_gpt5_temp:
        temperatura = st.slider(
            "Temperatura do modelo",
            min_value=0.0,
            max_value=1.0,
            value=0.5,
            step=0.05,
            help="Controla a criatividade da resposta. Valores mais baixos = respostas mais conservadoras.")
    else:
        st.sidebar.write("ðŸŒ¡ï¸ Temperatura: 1.0 (fixo para GPT-5)")
        temperatura = 1.0
    
    # Usar a mesma verificaÃ§Ã£o is_gpt5
    is_gpt5 = is_gpt5_temp
    
    if is_gpt5:
        st.sidebar.subheader("ðŸš€ ConfiguraÃ§Ãµes GPT-5")
        st.sidebar.info("â„¹ï¸ GPT-5 usa temperature fixa = 1.0 (nÃ£o configurÃ¡vel)")
        
        # Seletor de Verbosity
        verbosity = st.selectbox(
            "NÃ­vel de Verbosidade",
            options=["low", "medium", "high"],
            index=1,  # medium como padrÃ£o
            help="â€¢ Low: Respostas concisas\nâ€¢ Medium: Detalhamento balanceado\nâ€¢ High: Respostas detalhadas"
        )
        
        # OpÃ§Ã£o de Minimal Reasoning
        use_minimal_reasoning = st.checkbox(
            "Reasoning MÃ­nimo âš¡",
            value=False,
            help="Ativa reasoning mÃ­nimo para respostas mais rÃ¡pidas. Ideal para tarefas simples como classificaÃ§Ã£o ou extraÃ§Ã£o."
        )
        
        # OpÃ§Ãµes de Custom Tools
        st.sidebar.subheader("ðŸ”§ Ferramentas Personalizadas")
        use_custom_tools = st.checkbox(
            "Ativar Ferramentas Personalizadas",
            value=False,
            help="Permite uso de free-form function calling e CFG"
        )
        
        if use_custom_tools:
            tool_type = st.selectbox(
                "Tipo de Ferramenta",
                options=["SQL Generator", "Timestamp Generator", "Code Executor", "Custom"],
                help="Escolha o tipo de ferramenta personalizada"
            )
    else:
        verbosity = "medium"
        use_minimal_reasoning = False
        use_custom_tools = False
        tool_type = None

    # modelos = list(limites_modelos.keys())
    modelo_selecionado = modelo_selecionado_key.split(":", 1)[1].strip()

    limite_tokens = obter_limite_tokens(modelo_selecionado)
    limite_tokens_formatado = f"{limite_tokens:,}".replace(",", ".")
    paginas = f'{int(tokens_para_paginas_a4(limite_tokens)):,}'.replace(
        ",", ".")
    st.sidebar.write(
        f"Limite de tokens para contexto: {limite_tokens_formatado} tokens â‰… {paginas} pÃ¡ginas A4")

    # Upload de arquivos para RAG
    arquivos = st.file_uploader(
        "Anexe arquivos para RAG (PDF, DOCX, Excel, TXT ou CSV)",
        type=['pdf', 'docx', 'xlsx', 'xls', 'txt', 'csv'],
        accept_multiple_files=True,
        key="file_uploader"  # Adicionado key para o file_uploader tambÃ©m, boa prÃ¡tica
    )

    st.divider()
    st.subheader("Carregar Conversa Salva")
    arquivos_salvos = gc.listar_conversas_salvas()

    if arquivos_salvos:
        # Adiciona uma opÃ§Ã£o "Nova Conversa" ou "NÃ£o carregar"
        opcoes_carregamento = ["-- Nova Conversa --"] + arquivos_salvos
        arquivo_selecionado_para_carregar = st.selectbox(
            "Escolha uma conversa para carregar:",
            options=opcoes_carregamento,
            index=0,  # PadrÃ£o para "Nova Conversa"
            key="selectbox_carregar_conversa"
        )

        # BotÃ£o para carregar sÃ³ aparece se um arquivo real for selecionado
        if arquivo_selecionado_para_carregar != "-- Nova Conversa --":
            if st.button("Carregar Conversa Selecionada", key="carregar_json_selecionado"):
                caminho_completo = os.path.join(
                    gc.CONVERSATIONS_DIR, arquivo_selecionado_para_carregar)
                dados_conversa_carregada = gc.carregar_conversa_json(
                    caminho_completo)

                if dados_conversa_carregada:
                    # Limpa o histÃ³rico atual antes de carregar um novo
                    limpar_historico()  # Sua funÃ§Ã£o limpar_historico()
                    st.session_state["mensagens"] = dados_conversa_carregada.get(
                        "mensagens", [])
                    st.session_state[
                        "modelo_carregado_info"] = f"Conversa carregada: {arquivo_selecionado_para_carregar}"
                    # VocÃª pode querer definir o selectbox do modelo para o modelo_da_conversa se ele existir
                    # Isso pode ser complexo devido Ã  forma como o selectbox Ã© preenchido
                    st.rerun()
    else:
        st.write("Nenhuma conversa salva encontrada.")

    # Exibir informaÃ§Ã£o do modelo da conversa carregada, se houver
    if "modelo_carregado_info" in st.session_state and st.session_state["modelo_carregado_info"]:
        st.info(st.session_state["modelo_carregado_info"])


# Carregar o modelo selecionado apÃ³s a escolha do usuÃ¡rio na sidebar
# Usa a variÃ¡vel do selectbox corrigido (modelo_selecionado_key)
model_info = todos_modelos[modelo_selecionado_key]
# Extrai o nome real do modelo removendo o prefixo "Local: " ou "OpenAI: "
model_name = modelo_selecionado_key.split(
    ": ")[1] if ": " in modelo_selecionado_key else modelo_selecionado_key

# Carrega a instÃ¢ncia do LLM, passando a temperatura
# A funÃ§Ã£o load_model agora tem um valor padrÃ£o para temperature, mas passamos o valor do slider
llm = load_model(model_name, model_info, temperature=temperatura)


# ========== ExtraÃ§Ã£o dos arquivos enviados ==========
def ler_arquivo(uploaded_file):
    """LÃª o conteÃºdo de um arquivo enviado, suportando vÃ¡rios formatos."""
    try:
        if uploaded_file.name.endswith('.txt'):
            # Usa with para garantir que o arquivo seja fechado corretamente
            # Use getvalue() para arquivos em memÃ³ria
            return uploaded_file.getvalue().decode('utf-8')
        elif uploaded_file.name.endswith('.pdf'):
            reader = PdfReader(uploaded_file)
            text = ""
            for page in reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
            return text
        elif uploaded_file.name.endswith('.docx'):
            doc_file = docx.Document(uploaded_file)
            return "\n".join([p.text for p in doc_file.paragraphs])
        elif uploaded_file.name.endswith(('.xlsx', '.xls')):
            # LÃª a primeira planilha por padrÃ£o
            df = pd.read_excel(uploaded_file)
            return df.to_string()
        elif uploaded_file.name.endswith('.csv'):
            # Tente ler com utf-8, se falhar, tente latin1
            try:
                df = pd.read_csv(uploaded_file, encoding='utf-8')
            except UnicodeDecodeError:
                df = pd.read_csv(uploaded_file, encoding='latin1')
            return df.to_string()
        else:
            return "Tipo de arquivo nÃ£o suportado"
    except Exception as e:
        st.error(f"Erro ao ler o arquivo {uploaded_file.name}: {e}")
        return ""


def referencia_ao_anexo(pergunta):
    termos = [
        "no anexo", "no arquivo", "no documento", "nos anexos", "nos arquivos",
        "segundo o anexo", "de acordo com o anexo", "conforme o anexo", "conforme o documento anexo",
        "anexo diz", "arquivo diz", "conforme arquivo anexo", "documento diz", "anexo", "arquivo", "documento", "planilha", "csv", "txt"
    ]
    pergunta_lower = pergunta.lower()
    # Procura por palavras-chave seguidas de preposiÃ§Ã£o ou no meio da frase
    return any(
        termo in pergunta_lower
        for termo in termos
    )


# Exibir mensagens anteriores do histÃ³rico
for tipo, conteudo in st.session_state["mensagens"]:
    with st.chat_message(tipo):
        st.markdown(conteudo)

# Entrada do usuÃ¡rio via chat_input
prompt = st.chat_input("Mande sua mensagem para a IA...")

# Exibir arquivos enviados e seus conteÃºdos resumidos
textos_arquivos = []
if arquivos:
    st.markdown("### Arquivos enviados:")
    for arquivo in arquivos:
        st.write(f"- **{arquivo.name}**")
        texto = ler_arquivo(arquivo)
        textos_arquivos.append(texto)
        # Mostra sÃ³ o comeÃ§o do texto para nÃ£o sobrecarregar a interface
        # Adicionado verificaÃ§Ã£o para garantir que o texto nÃ£o Ã© None ou vazio
        if texto:
            st.write(texto[:500] + ("..." if len(texto) > 500 else ""))
        else:
            st.write("ConteÃºdo nÃ£o lido ou vazio.")

# BotÃ£o para limpar o histÃ³rico (exibido somente se houver mensagens ou prompt)
if prompt or st.session_state["mensagens"]:
    # Use um container para o botÃ£o para melhor organizaÃ§Ã£o
    with st.container():
        # Adicionado key Ãºnica para o botÃ£o Limpar HistÃ³rico
        if st.button("Limpar HistÃ³rico", key="limpar_historico_button"):
            limpar_historico()
            st.rerun()  # Recarrega a pÃ¡gina para limpar a interface

# Processar o prompt do usuÃ¡rio se houver e o LLM estiver carregado
if prompt and llm:

    if "selectbox_carregar_conversa" in st.session_state and \
       st.session_state.selectbox_carregar_conversa == "-- Nova Conversa --" and \
       "modelo_carregado_info" in st.session_state:
        # Limpa a info se iniciando nova conversa
        del st.session_state["modelo_carregado_info"]

    # Adiciona mensagem do usuÃ¡rio ao histÃ³rico da sessÃ£o
    st.session_state["mensagens"].append(("human", prompt))

    # Exibe a mensagem do usuÃ¡rio imediatamente na interface
    with st.chat_message("human"):
        st.markdown(prompt)

    # --- PREPARAÃ‡ÃƒO DAS MENSAGENS PARA O LLM USANDO OBJETOS LANGCHAIN ---
    mensagens_para_llm = []
    for tipo, conteudo in st.session_state["mensagens"]:
        if tipo == "human":
            mensagens_para_llm.append(HumanMessage(content=conteudo))
        elif tipo == "ai":
            mensagens_para_llm.append(AIMessage(content=conteudo))

    # Obter limite de tokens para o modelo
    limite_tokens = obter_limite_tokens(modelo_selecionado)

    # Verifica se a lista nÃ£o estÃ¡ vazia e contÃ©m algum texto
    # Verifica se a lista nÃ£o estÃ¡ vazia e contÃ©m algum texto
    if textos_arquivos and any(textos_arquivos):
        contexto_arquivos = "\n\n".join(filter(None, textos_arquivos))
        contexto_limitado = limitar_texto_por_tokens(
            contexto_arquivos, limite_tokens)

        # SÃ³ insere o contexto dos anexos se a pergunta mencionar o anexo
        if referencia_ao_anexo(prompt):
            system_prompt = (
                "VocÃª Ã© um assistente de IA. Utilize EXCLUSIVAMENTE as informaÃ§Ãµes (extraÃ­das de arquivos enviados pelo usuÃ¡rio) para responder Ã  pergunta. "
                "Se nÃ£o encontrar a resposta, diga 'NÃ£o encontrei essa informaÃ§Ã£o nos arquivos'.\n\n"
                "Foque EXCLUSIVAMENTE nas informaÃ§Ãµes dos arquivos enviados. "
                "NÃ£o use informaÃ§Ãµes externas ou conhecimento prÃ©vio.\n\n"
                f"INFORMAÃ‡Ã•ES DOS ARQUIVOS:\n{contexto_limitado}\n\n"
            )
            mensagens_para_llm.insert(0, SystemMessage(content=system_prompt))
        else:
            # Prompt padrÃ£o, sem contexto dos arquivos
            system_prompt = (
                "VocÃª Ã© um assistente de IA. Responda normalmente ao usuÃ¡rio."
            )
            mensagens_para_llm.insert(0, SystemMessage(content=system_prompt))
    else:
        # Prompt padrÃ£o, sem contexto dos arquivos
        system_prompt = (
            "Responda normalmente ao usuÃ¡rio."
        )
        mensagens_para_llm.insert(0, SystemMessage(content=system_prompt))

    # --- FIM DA PREPARAÃ‡ÃƒO DAS MENSAGENS ---

    # BotÃ£o para interromper geraÃ§Ã£o
    # Usando um container e key Ãºnica para o botÃ£o Parar geraÃ§Ã£o
    col_stop = st.empty()
    # Adicionado key Ãºnica para o botÃ£o Parar geraÃ§Ã£o
    if col_stop.button("Parar geraÃ§Ã£o", key="stop_button"):
        st.session_state["parar_geracao"] = True

    # Antes do loop de streaming, resetar a flag parar_geracao para a nova interaÃ§Ã£o
    st.session_state["parar_geracao"] = False

    # Criar espaÃ§o para a resposta da IA
    with st.chat_message("ai"):
        resposta_placeholder = st.empty()
        # Reinicia a resposta completa para a nova interaÃ§Ã£o
        st.session_state["resposta_completa"] = ""

    # Cria barra de progresso vazia (agora dentro do bloco if prompt and llm)
    progress_bar = st.progress(0)

    # Bloco principal para gerar a resposta da IA com spinner e barra de progresso
    with st.spinner("IA gerando resposta..."):
        progress = 0
        direction = 1  # Para animaÃ§Ã£o da barra de progresso

        try:
            # Verifica se Ã© GPT-5 para usar novos recursos
            if isinstance(llm, GPT5Handler):
                # Prepara o histÃ³rico no formato correto
                conversation_history = []
                for tipo, conteudo in st.session_state["mensagens"][:-1]:  # Exclui a mensagem atual
                    role = "user" if tipo == "human" else "assistant"
                    conversation_history.append({"role": role, "content": conteudo})
                
                # Verifica se deve usar ferramentas personalizadas
                if use_custom_tools and tool_type:
                    if tool_type == "SQL Generator":
                        result = llm.create_sql_query_with_dialect(
                            query_description=prompt,
                            dialect="postgresql",
                            model=model_name
                        )
                        if "tool_call" in result:
                            st.session_state["resposta_completa"] = f"**SQL Gerado:**\n```sql\n{result['tool_call']['input']}\n```\n\n**ExplicaÃ§Ã£o:**\n{result.get('response', 'Query SQL gerada com sucesso.')}"
                        else:
                            st.session_state["resposta_completa"] = result.get('response', 'Erro na geraÃ§Ã£o SQL')
                    
                    elif tool_type == "Timestamp Generator":
                        result = llm.create_response_with_custom_tool(
                            input_message=prompt,
                            tool_name="timestamp_generator",
                            tool_description="Gera timestamp no formato YYYY-MM-DD HH:MM",
                            model=model_name,
                            grammar_definition=r"^\d{4}-(0[1-9]|1[0-2])-(0[1-9]|[12]\d|3[01]) (?:[01]\d|2[0-3]):[0-5]\d$",
                            grammar_syntax="regex"
                        )
                        if "tool_call" in result:
                            st.session_state["resposta_completa"] = f"**Timestamp:**\n{result['tool_call']['input']}\n\n{result.get('response', '')}"
                        else:
                            st.session_state["resposta_completa"] = result.get('response', 'Erro na geraÃ§Ã£o do timestamp')
                    
                    else:
                        # Usa resposta padrÃ£o para outros tipos
                        if use_minimal_reasoning:
                            result = llm.create_response_with_minimal_reasoning(
                                input_message=prompt,
                                model=model_name,
                                conversation_history=conversation_history
                            )
                        else:
                            result = llm.create_response_with_verbosity(
                                input_message=prompt,
                                model=model_name,
                                verbosity=verbosity,
                                conversation_history=conversation_history
                            )
                        st.session_state["resposta_completa"] = result.get('response', 'Erro na resposta')
                
                else:
                    # Usa resposta padrÃ£o com verbosity ou minimal reasoning
                    if use_minimal_reasoning:
                        result = llm.create_response_with_minimal_reasoning(
                            input_message=prompt,
                            model=model_name,
                            conversation_history=conversation_history
                        )
                        # Adiciona indicador de reasoning mÃ­nimo
                        usage_info = result.get('usage', {})
                        tokens_info = f" (âš¡ {usage_info.get('output_tokens', 'N/A')} tokens)"
                        st.session_state["resposta_completa"] = result.get('response', 'Erro na resposta') + f"\n\n*Resposta rÃ¡pida{tokens_info}*"
                    else:
                        result = llm.create_response_with_verbosity(
                            input_message=prompt,
                            model=model_name,
                            verbosity=verbosity,
                            conversation_history=conversation_history
                        )
                        # Adiciona informaÃ§Ãµes de verbosity e tokens
                        usage_info = result.get('usage', {})
                        tokens_info = f" ({usage_info.get('output_tokens', 'N/A')} tokens)"
                        verbosity_emoji = {"low": "ðŸ“", "medium": "ðŸ“„", "high": "ðŸ“š"}
                        st.session_state["resposta_completa"] = result.get('response', 'Erro na resposta') + f"\n\n*{verbosity_emoji.get(verbosity, 'ðŸ“„')} Verbosity: {verbosity}{tokens_info}*"
                
                # Simula progresso para GPT-5 (resposta nÃ£o Ã© streaming)
                for i in range(10):
                    if st.session_state["parar_geracao"]:
                        st.session_state["resposta_completa"] += "\n\n**GeraÃ§Ã£o interrompida pelo usuÃ¡rio.**"
                        break
                    progress = (i + 1) * 0.1
                    progress_bar.progress(progress)
                    time.sleep(0.1)
                
                # Exibe a resposta final
                resposta_placeholder.markdown(st.session_state["resposta_completa"])
                
            else:
                # CÃ³digo original para modelos nÃ£o-GPT5
                for chunk in llm.stream(mensagens_para_llm):
                    # Verifica se a flag de parada foi ativada
                    if st.session_state["parar_geracao"]:
                        st.session_state["resposta_completa"] += "\n\n**GeraÃ§Ã£o interrompida pelo usuÃ¡rio.**"
                        resposta_placeholder.markdown(
                            st.session_state["resposta_completa"])
                        break  # Sai do loop de streaming

                    # Verifica se o chunk tem conteÃºdo e se Ã© do tipo esperado (AIMessageChunk ou similar)
                    if hasattr(chunk, 'content') and chunk.content is not None:
                        st.session_state["resposta_completa"] += chunk.content
                        # Atualiza o placeholder com a resposta parcial e um cursor
                        resposta_placeholder.markdown(
                            st.session_state["resposta_completa"] + "â–Œ")

                    # Atualiza barra de progresso animada (simulada)
                    progress += direction * 0.05  # Ajuste a velocidade da animaÃ§Ã£o
                    if progress >= 1:
                        direction = -1
                    elif progress <= 0:
                        direction = 1
                    progress_bar.progress(progress)
                    time.sleep(0.02)  # Pausa curta para a animaÃ§Ã£o ser visÃ­vel

        except Exception as e:
            st.error(f"Erro ao gerar resposta do modelo: {e}")
            st.session_state["resposta_completa"] += "\n\nDesculpe, ocorreu um erro ao gerar a resposta."
            resposta_placeholder.markdown(
                st.session_state["resposta_completa"])

        finally:
            # Garante que a barra de progresso e o spinner sejam removidos ao final
            progress_bar.empty()
            # Exibe a resposta final completa (sem o cursor)
            resposta_placeholder.markdown(
                st.session_state["resposta_completa"])
            # Reseta a flag de parada apÃ³s a geraÃ§Ã£o (completa ou interrompida)
            st.session_state["parar_geracao"] = False

    # Adicionar a resposta completa da IA ao histÃ³rico da sessÃ£o
    # Isso Ã© feito APENAS apÃ³s a resposta completa ser recebida/streamada ou interrompida
    # Garante que sÃ³ adiciona se houver algum conteÃºdo gerado
    if st.session_state["resposta_completa"] and not st.session_state["resposta_completa"].endswith("**GeraÃ§Ã£o interrompida pelo usuÃ¡rio.**"):
        st.session_state["mensagens"].append(
            ("ai", st.session_state["resposta_completa"]))
    elif st.session_state["resposta_completa"].endswith("**GeraÃ§Ã£o interrompida pelo usuÃ¡rio.**"):
        # Adiciona a resposta parcial com a mensagem de interrupÃ§Ã£o ao histÃ³rico
        st.session_state["mensagens"].append(
            ("ai", st.session_state["resposta_completa"]))


# Exibir botÃµes de salvar SOMENTE se houver mensagens no histÃ³rico
if st.session_state["mensagens"]:
    col1, col2, col3 = st.columns(3)

    # Verifica se hÃ¡ pelo menos uma mensagem da IA para salvar
    if any(tipo == "ai" for tipo, _ in st.session_state["mensagens"]):
        with col1:
            # BotÃ£o para salvar toda a conversa (apenas as respostas da IA)
            # Adicionado key Ãºnica para o botÃ£o Salvar toda a conversa
            if st.button("Salvar toda a conversa", key="salvar_toda_conversa"):
                respostas_ia = [
                    conteudo for tipo, conteudo in st.session_state["mensagens"] if tipo == "ai"]
                if respostas_ia:
                    try:
                        # Salva em modo 'append' ('a') para adicionar a conversas anteriores
                        with open("conversa_completa.txt", "a", encoding="utf-8") as f:
                            for i, resposta in enumerate(respostas_ia):
                                # Opcional: numera as respostas
                                f.write(f"Resposta IA {i+1}:\n")
                                f.write(resposta.strip() + "\n\n")
                        st.success(
                            "Toda a conversa foi salva com sucesso em conversa_completa.txt!")
                    except Exception as e:
                        st.error(f"Erro ao salvar a conversa completa: {e}")

        with col2:
            # BotÃ£o para salvar a Ãºltima resposta da IA
            # Adicionado key Ãºnica para o botÃ£o Salvar Ãºltima resposta
            if st.button("Salvar Ãºltima resposta", key="salvar_ultima_resposta"):
                ultimas_respostas = [
                    msg[1] for msg in st.session_state["mensagens"] if msg[0] == "ai"]
                if ultimas_respostas:
                    try:
                        # Salva em modo 'write' ('w') para sobrescrever o arquivo anterior
                        with open("ultima_resposta.txt", "w", encoding="utf-8") as f:
                            f.write(ultimas_respostas[-1].strip())
                        st.success(
                            "Ãšltima resposta salva com sucesso em ultima_resposta.txt!")
                    except Exception as e:
                        st.error(f"Erro ao salvar a Ãºltima resposta: {e}")

    # BotÃ£o para salvar a conversa atual em JSON
    with col3:
        if st.button("Salvar Conversa Atual (JSON)", key="salvar_json_conversa_atual"):
            modelo_atual = st.session_state.get(
                "modelo_selecionado_key", "desconhecido")
            # Passa o nome do modelo para ser salvo nos metadados
            gc.salvar_conversa_json(
                st.session_state["mensagens"], modelo_usado=modelo_atual)
